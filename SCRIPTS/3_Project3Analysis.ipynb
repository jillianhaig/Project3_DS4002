{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jillianhaig/Project3_DS4002/blob/main/SCRIPTS/3_Project3Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This code runs our model, first connecting to google drive and importing the\n",
        "# images into GitHub through Google Drive, since GitHub is not large enough to store the images"
      ],
      "metadata": {
        "id": "9MZ0axzBNsX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJYvZN8CUpnr",
        "outputId": "6e5e58da-baaa-4fe2-b74f-d8b9ed594f4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Dataset unzipped to: /content/vehicleimages\n",
            "                                       image_path      label\n",
            "0   /content/vehicleimages/hatchback/PHOTO_97.jpg  hatchback\n",
            "1  /content/vehicleimages/hatchback/PHOTO_464.jpg  hatchback\n",
            "2  /content/vehicleimages/hatchback/PHOTO_393.jpg  hatchback\n",
            "3  /content/vehicleimages/hatchback/PHOTO_480.jpg  hatchback\n",
            "4  /content/vehicleimages/hatchback/PHOTO_574.jpg  hatchback\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Mount Google Drive to access the dataset\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the zip file on Google Drive (need to change for your path)\n",
        "zip_file_path = '/content/drive/My Drive/vehicleimages.zip'\n",
        "\n",
        "# Directory where you want to extract the files\n",
        "extract_to_path = '/content/vehicleimages'\n",
        "\n",
        "# Unzip the dataset\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_to_path)\n",
        "\n",
        "print(f\"Dataset unzipped to: {extract_to_path}\")\n",
        "\n",
        "image_paths = []\n",
        "labels = []\n",
        "\n",
        "vehicle_types = os.listdir(extract_to_path)\n",
        "\n",
        "# For each subdirectory, get image paths\n",
        "for vehicle_type in vehicle_types:\n",
        "    vehicle_folder = os.path.join(extract_to_path, vehicle_type)\n",
        "\n",
        "    if os.path.isdir(vehicle_folder):\n",
        "        for img_file in os.listdir(vehicle_folder):\n",
        "            if img_file.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp')):\n",
        "                image_path = os.path.join(vehicle_folder, img_file)\n",
        "                image_paths.append(image_path)\n",
        "                labels.append(vehicle_type)\n",
        "\n",
        "df = pd.DataFrame({'image_path': image_paths, 'label': labels})\n",
        "\n",
        "# Creates dictionary to create dummy variables in integer format\n",
        "category_map = {\n",
        "    'other': 0,\n",
        "    'hatchback': 1,\n",
        "    'suv': 2,\n",
        "    'pickup': 3,\n",
        "    'sedan': 4\n",
        "}\n",
        "\n",
        "# Apply the dictionary to the 'label' column\n",
        "df['label'] = df['label'].map(category_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CNN Model Construction and Compiling"
      ],
      "metadata": {
        "id": "MmuTtxwTcI8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming `df` is a pandas DataFrame with file paths to images and their corresponding labels\n",
        "# df should contain a column 'file_path' with paths to images and a column 'label' with corresponding class labels\n",
        "\n",
        "# Load the pre-trained ResNet50 model\n",
        "resnet50_base = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the layers of ResNet50 to prevent updating during initial training\n",
        "resnet50_base.trainable = False\n",
        "\n",
        "# Define the custom classification head\n",
        "model = models.Sequential([\n",
        "    resnet50_base,  # Pre-trained ResNet50 model\n",
        "    layers.GlobalAveragePooling2D(),  # Global Average Pooling layer\n",
        "    layers.Dense(512, activation='relu'),  # Fully connected layer\n",
        "    layers.Dropout(0.5),  # Dropout layer to prevent overfitting\n",
        "    layers.Dense(5, activation='softmax')  # Output layer with 5 units for 5 vehicle classes\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Display model architecture\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "BEfIkTJucMmJ",
        "outputId": "6a525545-a0ee-4b49-fda3-1410a8bc5092",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m)          │      \u001b[38;5;34m23,587,712\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling2d_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │       \u001b[38;5;34m1,049,088\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                   │           \u001b[38;5;34m2,565\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)          │      <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ global_average_pooling2d_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,049,088</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,565</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,639,365\u001b[0m (93.99 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,639,365</span> (93.99 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,051,653\u001b[0m (4.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,051,653</span> (4.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,587,712\u001b[0m (89.98 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> (89.98 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial Model Training"
      ],
      "metadata": {
        "id": "-avYcC9ucX5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Preprocessing the data (resizing and normalizing images)\n",
        "image_size = (224, 224)\n",
        "\n",
        "# Create an ImageDataGenerator to load images with augmentation for training and validation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# Example of file paths and labels:\n",
        "# df['image_path'] - list of paths to images\n",
        "# df['label'] - list of integer class labels (0 to 4 for 5 vehicle types)\n",
        "\n",
        "# Train-test split (90-10 split)\n",
        "train_paths, val_paths, train_labels, val_labels = train_test_split(df['image_path'], df['label'], test_size=0.1, stratify=df['label'])\n",
        "\n",
        "# Create ImageDataGenerators for training and validation\n",
        "train_generator = train_datagen.flow_from_dataframe(\n",
        "    dataframe=df.loc[train_paths.index],\n",
        "    directory='/content',\n",
        "    x_col='image_path',\n",
        "    y_col='label',\n",
        "    target_size=image_size,\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_dataframe(\n",
        "    dataframe=df.loc[val_paths.index],\n",
        "    directory='/content',\n",
        "    x_col='image_path',\n",
        "    y_col='label',\n",
        "    target_size=image_size,\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=50,  # Or any number you prefer, but EarlyStopping will stop early if necessary\n",
        "    validation_data=validation_generator,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stopping]  # Add EarlyStopping\n",
        ")"
      ],
      "metadata": {
        "id": "-ePL7KJCcbpt",
        "outputId": "dce867b6-871b-4150-99f1-2cceb7313d2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3920 validated image filenames belonging to 5 classes.\n",
            "Found 436 validated image filenames belonging to 5 classes.\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 429ms/step - accuracy: 0.3400 - loss: 1.5115 - val_accuracy: 0.3326 - val_loss: 1.5118\n",
            "Epoch 2/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 433ms/step - accuracy: 0.3403 - loss: 1.5112 - val_accuracy: 0.3716 - val_loss: 1.4847\n",
            "Epoch 3/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 420ms/step - accuracy: 0.3394 - loss: 1.4994 - val_accuracy: 0.3716 - val_loss: 1.4699\n",
            "Epoch 4/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 421ms/step - accuracy: 0.3495 - loss: 1.4870 - val_accuracy: 0.3830 - val_loss: 1.4624\n",
            "Epoch 5/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 425ms/step - accuracy: 0.3617 - loss: 1.4802 - val_accuracy: 0.3234 - val_loss: 1.4774\n",
            "Epoch 6/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 423ms/step - accuracy: 0.3658 - loss: 1.4686 - val_accuracy: 0.3739 - val_loss: 1.4493\n",
            "Epoch 7/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 414ms/step - accuracy: 0.3664 - loss: 1.4655 - val_accuracy: 0.4037 - val_loss: 1.4003\n",
            "Epoch 8/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 412ms/step - accuracy: 0.3902 - loss: 1.4573 - val_accuracy: 0.4037 - val_loss: 1.4017\n",
            "Epoch 9/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 408ms/step - accuracy: 0.3863 - loss: 1.4551 - val_accuracy: 0.3693 - val_loss: 1.4295\n",
            "Epoch 10/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 420ms/step - accuracy: 0.3693 - loss: 1.4458 - val_accuracy: 0.4060 - val_loss: 1.3818\n",
            "Epoch 11/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 423ms/step - accuracy: 0.3834 - loss: 1.4402 - val_accuracy: 0.4197 - val_loss: 1.3552\n",
            "Epoch 12/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 431ms/step - accuracy: 0.3931 - loss: 1.4338 - val_accuracy: 0.3693 - val_loss: 1.3926\n",
            "Epoch 13/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 422ms/step - accuracy: 0.3958 - loss: 1.4282 - val_accuracy: 0.3830 - val_loss: 1.4004\n",
            "Epoch 14/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 421ms/step - accuracy: 0.3819 - loss: 1.4233 - val_accuracy: 0.4220 - val_loss: 1.3594\n",
            "Epoch 15/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 423ms/step - accuracy: 0.4070 - loss: 1.4193 - val_accuracy: 0.3991 - val_loss: 1.3828\n",
            "Epoch 16/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 423ms/step - accuracy: 0.3995 - loss: 1.4239 - val_accuracy: 0.4564 - val_loss: 1.3103\n",
            "Epoch 17/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 408ms/step - accuracy: 0.3854 - loss: 1.4315 - val_accuracy: 0.4450 - val_loss: 1.3400\n",
            "Epoch 18/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 416ms/step - accuracy: 0.3920 - loss: 1.4276 - val_accuracy: 0.4128 - val_loss: 1.3563\n",
            "Epoch 19/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 419ms/step - accuracy: 0.4104 - loss: 1.4108 - val_accuracy: 0.4243 - val_loss: 1.3540\n",
            "Epoch 20/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 426ms/step - accuracy: 0.4040 - loss: 1.4209 - val_accuracy: 0.4908 - val_loss: 1.2591\n",
            "Epoch 21/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 425ms/step - accuracy: 0.4047 - loss: 1.4121 - val_accuracy: 0.4885 - val_loss: 1.2745\n",
            "Epoch 22/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 419ms/step - accuracy: 0.4032 - loss: 1.4024 - val_accuracy: 0.4794 - val_loss: 1.2980\n",
            "Epoch 23/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 421ms/step - accuracy: 0.4076 - loss: 1.3980 - val_accuracy: 0.4404 - val_loss: 1.3240\n",
            "Epoch 24/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 409ms/step - accuracy: 0.4001 - loss: 1.3988 - val_accuracy: 0.4702 - val_loss: 1.3108\n",
            "Epoch 25/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 406ms/step - accuracy: 0.4055 - loss: 1.3802 - val_accuracy: 0.4748 - val_loss: 1.2854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning the Model"
      ],
      "metadata": {
        "id": "mU7BU74zdcEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze the top layers of ResNet50 for fine-tuning\n",
        "for layer in resnet50_base.layers[-10:]:  # Unfreeze the last 10 layers of ResNet50\n",
        "    layer.trainable = True\n",
        "\n",
        "# Recompile the model after unfreezing some layers\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Fine-tune the model\n",
        "history_finetune = model.fit(\n",
        "    train_generator,\n",
        "    epochs=50,\n",
        "    validation_data=validation_generator,\n",
        "    verbose=1,\n",
        "    callbacks=[early_stopping]  # Add EarlyStopping\n",
        ")"
      ],
      "metadata": {
        "id": "43y--0IcdgZr",
        "outputId": "675e8bfe-2539-46cf-fd6c-a07bea784759",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 495ms/step - accuracy: 0.3342 - loss: 3.1483 - val_accuracy: 0.3188 - val_loss: 1.3977\n",
            "Epoch 2/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 430ms/step - accuracy: 0.4159 - loss: 1.3872 - val_accuracy: 0.4702 - val_loss: 1.4313\n",
            "Epoch 3/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 425ms/step - accuracy: 0.4463 - loss: 1.3427 - val_accuracy: 0.4151 - val_loss: 1.4516\n",
            "Epoch 4/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 433ms/step - accuracy: 0.4670 - loss: 1.3071 - val_accuracy: 0.5138 - val_loss: 1.2348\n",
            "Epoch 5/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 431ms/step - accuracy: 0.4707 - loss: 1.2720 - val_accuracy: 0.5688 - val_loss: 1.1278\n",
            "Epoch 6/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 432ms/step - accuracy: 0.4849 - loss: 1.2633 - val_accuracy: 0.5711 - val_loss: 1.1075\n",
            "Epoch 7/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 430ms/step - accuracy: 0.4799 - loss: 1.2825 - val_accuracy: 0.5092 - val_loss: 1.2603\n",
            "Epoch 8/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 433ms/step - accuracy: 0.5079 - loss: 1.2334 - val_accuracy: 0.5573 - val_loss: 1.1147\n",
            "Epoch 9/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 432ms/step - accuracy: 0.5046 - loss: 1.2046 - val_accuracy: 0.5711 - val_loss: 1.0962\n",
            "Epoch 10/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 436ms/step - accuracy: 0.4971 - loss: 1.2189 - val_accuracy: 0.6009 - val_loss: 0.9696\n",
            "Epoch 11/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 430ms/step - accuracy: 0.5193 - loss: 1.1830 - val_accuracy: 0.6216 - val_loss: 0.9206\n",
            "Epoch 12/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 430ms/step - accuracy: 0.5196 - loss: 1.1988 - val_accuracy: 0.6170 - val_loss: 0.8906\n",
            "Epoch 13/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 421ms/step - accuracy: 0.5345 - loss: 1.1646 - val_accuracy: 0.6078 - val_loss: 0.9641\n",
            "Epoch 14/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 425ms/step - accuracy: 0.5403 - loss: 1.1616 - val_accuracy: 0.6239 - val_loss: 0.9332\n",
            "Epoch 15/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 412ms/step - accuracy: 0.5374 - loss: 1.1682 - val_accuracy: 0.6193 - val_loss: 0.9180\n",
            "Epoch 16/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 416ms/step - accuracy: 0.5563 - loss: 1.1121 - val_accuracy: 0.5986 - val_loss: 0.9689\n",
            "Epoch 17/50\n",
            "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 415ms/step - accuracy: 0.5608 - loss: 1.1264 - val_accuracy: 0.6216 - val_loss: 0.9283\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation and Conclusion"
      ],
      "metadata": {
        "id": "lzucn-4Qdhnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "# Assuming you already have 'df', 'y', 'train_datagen', 'validation_datagen', 'model', 'early_stopping', and 'image_size'\n",
        "\n",
        "# K-fold cross-validation\n",
        "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "accuracies = []\n",
        "f1_scores = []\n",
        "\n",
        "for train_idx, test_idx in kfold.split(df['image_path'], df['label']):\n",
        "    # Split the data into training and testing\n",
        "    train_paths, test_paths = df['image_path'].iloc[train_idx], df['image_path'].iloc[test_idx]\n",
        "    train_labels, test_labels = df['label'].iloc[train_idx], df['label'].iloc[test_idx]\n",
        "\n",
        "    # Prepare generators for training and testing\n",
        "    train_generator = train_datagen.flow_from_dataframe(\n",
        "        dataframe=df.loc[train_idx],\n",
        "        directory='/content',\n",
        "        x_col='image_path',\n",
        "        y_col='label',\n",
        "        target_size=image_size,\n",
        "        batch_size=32,\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "\n",
        "    test_generator = validation_datagen.flow_from_dataframe(\n",
        "        dataframe=df.loc[test_idx],\n",
        "        directory='/content',\n",
        "        x_col='image_path',\n",
        "        y_col='label',\n",
        "        target_size=image_size,\n",
        "        batch_size=32,\n",
        "        class_mode='categorical'\n",
        "    )\n",
        "\n",
        "    # Train the model on the current fold\n",
        "    model.fit(train_generator, epochs=50, validation_data=test_generator, callbacks=[early_stopping])\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    y_pred = model.predict(test_generator, verbose=1)\n",
        "    y_true = test_labels\n",
        "\n",
        "    # Calculate accuracy and F1 score\n",
        "    acc = accuracy_score(y_true.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "    f1 = f1_score(y_true.argmax(axis=1), y_pred.argmax(axis=1), average='weighted')\n",
        "\n",
        "    accuracies.append(acc)\n",
        "    f1_scores.append(f1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "hLdxUayNekq-",
        "outputId": "8c7139d7-bbcd-47d9-f64d-fe7201ebd55e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f492acb1c387>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mf1_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Split the data into training and testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtrain_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image_path'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results and Visualization"
      ],
      "metadata": {
        "id": "NLEnnCLt3ydq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print results\n",
        "print(\"Individual Accuracy: \", accuracies)\n",
        "print(\"Individual F1-Score: \", f1_scores)\n",
        "print(\"Mean Accuracy: \", np.mean(accuracies))\n",
        "print(\"Mean F1-Score: \", np.mean(f1_scores))"
      ],
      "metadata": {
        "id": "wL4HCfKv31pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ROC Graph\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "y_true_all = []\n",
        "y_probs_all = []\n",
        "\n",
        "# Store true labels and predicted probabilities for ROC curve\n",
        "y_true_all.append(y_true)\n",
        "y_probs_all.append(y_pred)\n",
        "\n",
        "# Convert lists of true labels and predicted probabilities to arrays\n",
        "y_true_all = np.concatenate(y_true_all, axis=0)\n",
        "y_probs_all = np.concatenate(y_probs_all, axis=0)\n",
        "\n",
        "# Binarize the true labels (multi-class to binary for each class)\n",
        "lb = LabelBinarizer()\n",
        "y_true_bin = lb.fit_transform(y_true_all)\n",
        "\n",
        "# Plot ROC curve for each class\n",
        "n_classes = y_true_bin.shape[1]\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "for i in range(n_classes):\n",
        "    fpr, tpr, _ = roc_curve(y_true_bin[:, i], y_probs_all[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, lw=2, label=f'Class {i} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "# Plot the diagonal line (random classifier)\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', lw=2)\n",
        "\n",
        "# Formatting the plot\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve for Multi-Class Classification')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZprDzTyF34lb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}